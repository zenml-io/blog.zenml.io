---
layout: post
author: Hamza Tahir
title: Spot the difference in ML costs
description: "Spot instances are a great option for anyone training machine
learning models; they're much cheaper than other on-demand options, albeit with
some drawbacks."
category: mlops
tags: cloud machine-learning zenml legacy
publish_date: January 28th, 2021
date: 2021-01-28T00:02:00Z
thumbnail: /assets/posts/12factor.svg
image:
  path: /assets/posts/12factors.png
  height: 1910
  width: 1000
---

**Last updated:** February 16, 2022.

Every organization at any scale understands that leveraging the public cloud is
a trade-off between convenience and cost. While cloud providers like Google,
Amazon and Microsoft have immensely reduced the barrier of entry for machine
learning, GPU costs are still at a premium.

There is an
[increasing fear in the machine learning community](https://venturebeat.com/2020/06/01/ai-machine-learning-openai-gpt-3-size-isnt-everything/)
that the true power of machine learning is still within the hands of the few.
The flagship example of this is OpenAI's massive GPT-3 model containing 175
billion parameters, a memory footprint of 350GB and reportedly costing at least
[$4.6 million](https://lambdalabs.com/blog/demystifying-gpt-3/) to train. The
trend also looks set to continue: Rumours consistently circulate regarding the
next generation GPT-4's size, with some estimates ranging in the order of
[**trillions** of parameters](https://www.metaculus.com/questions/4852/how-many-parameters-will-gpt-4-have-if-it-is-released-in-billions-of-parameters/).
Even with more efficient training techniques, these models will still cost in
the order of millions to train.

For the rest of the ML community, there is now an increasing reliance on their
secret weapon: Transfer learning. Just recently, the excellent
[HuggingFace](https://huggingface.co/) library
[announced a simple method](https://twitter.com/huggingface/status/1351560093658198022)
to fine-tune large-scale parameter models on a single cloud GPU. This gives hope
to ML practitioners that even if they are unable to train models from scratch,
utilizing the immense power of modern-day machine learning models is still
within reach.

## The need for the cloud

Whether training from scratch, or fine-tuning, it is still clear that public
cloud providers offer the most convenient path to provision and utilize compute
resources for most ML practitioners out there. However, even for fine-tuning
tasks or smaller models, these costs can quickly grow and become unmanageable.
For example, here is a simple breakdown of much it costs to train a machine
learning model on a relatively mid-tier configuration suitable for many machine
learning tasks:

```
| Provider | Configuration           | Cost ($/h)    |
|----------|-------------------------|---------------|
| GCP      | n1-standard-16 with P4  | 1.36          |
| Azure    | NV6 with M60            | 1.14          |
| AWS      | g3.4xlarge with M60     | 1.14          |
```

Bear in mind that the above costs are just for one training run. Most machine
learning projects go through much more experimentation phases, and these numbers
add up quickly. Therefore, most ML teams who do not have vast budgets on their
hands usually resort to sampling their datasets and portioning out big training
runs for when they are sure. This can be slow and tedious, not to mention hard
to coordinate. It can also lead to teams converging to the wrong results. If the
smaller, sampled datasets are not representative of the larger dataset, it can
lead to frustrating and diverging results as the models develop.

## Spot instances: A perfect fit for ML experimentation

Would it not be easy if ML practitioners had the luxury to launch experiments
without having to fret so much about the costs exploding over time? There might
be just be a solution, offered by all major cloud providers, and severely
underutilized by the machine learning community: **Preemptible/Spot instances.**

The word preemptible instance is largely a Google Cloud Platform term, while
spot instance is used by AWS/Azure. Whatever you call it, the concept is the
same: These instances cost a fraction of the cost of normal instances, and the
only catch is that there is no guarantee that the instance stays up all the
time. Usually, this means that within 24 hours the instance is shut down by the
provider.

These sorts of instances are a mechanism for the cloud providers to maximize the
utilization of all their resources at any given time. They are intended for
batched, non-critical workloads. Most trainings jobs for the majority of the
use-cases out there take less than 24 hours to complete. And even if the job is
interrupted before that happens, they can almost always be restarted from
checkpoints.

## Cost comparison: 80% cost reduction

Therefore, machine learning training fits the intended use of spot instances
perfectly. By using these instances, practitioners stand to gain massive cost
reductions. We have conducted a rough analysis across the three major cloud
providers to showcase the cost benefits. The raw data can be found
[here](https://docs.google.com/spreadsheets/d/1wErQviA3sI22fh3BscO4CMJyg6w1Qqi468O1bCxUFhc/edit?usp=sharing).
Feel free to share the doc and leave a comment if you find something to add.
Here is a snapshot, with the same configurations as before:

```
| Provider | Configuration           | Cost ($/h) | Spot Cost ($/h) | Savings |
|----------|-------------------------|------------|---------------------------|
| GCP      | n1-standard-16 with P4  | 1.36       | 0.38            | 72%     |
| Azure    | NV6 with M60            | 1.14       | 0.20            | 82%     |
| AWS      | g3.4xlarge with M60     | 1.14       | 0.34            | 70%     |
```

Note: All costs in the US region, AWS instance pricing as of January 28, 14:00
CET.

As can be seen, depending on the configuration, there is up to **82%** cost
reduction by using spot instances, with the average cost savings across multiple
cloud and configurations being roughly **74%**. This can equate to hundreds of
dollars worth of savings. Especially for hobbyists, smaller companies, or
smaller departments experimenting with Machine Learning, this may mean the
difference between getting a model deployed vs crashing and burning before
lift-off.

Using this technique is not new: Way back in 2018, the FastAI team
[trained ImageNet in 18 minutes with 16 AWS spot instances](https://www.fast.ai/2018/08/10/fastai-diu-imagenet/).
This cost $40 at the time, and was the most public display of the insane cost
benefits of spot instances in the community.

However, given the trend of increasingly big models, and the increasing adoption
of AI worldwide, I can only see the need for spot instance training increasing
over time. Given the dramatic difference in costs, it is almost a no-brainer to
use spot instance training as a primary mechanism for training, at least in the
experimentation phase.

## ZenML: A simple tool to orchestrate spot instance training

If youâ€™re looking for a head start for spot instance training, check out
[ZenML](https://github.com/zenml-io/zenml), an open-source MLOps framework for
reproducible machine learning. Running spot pipeline in ZenML, is as easy as :

```python
training_pipeline.run(
    backend=OrchestratorGCPBackend(
        preemptible=True,  # reduce costs by using preemptible (spot) instances
        machine_type='n1-standard-4',
        gpu='nvidia-tesla-k80',
        gpu_count=1,
    )
)
```

[See gist here](https://gist.github.com/htahir1/62dc4baa12560e8b88ce156f76aaab5f)

ZenML not only zips your code up to the instance, it also makes sure the right
CUDA-drivers are enabled to take advantage of the accelerator of your choice. It
provisions the instance, and spins it down when the pipeline is done. Not to
mention the other benefits of experiment tracking, versioning and metadata
management which is provided anyway by the framework. Give it a spin yourself: A
full code example can be found
[here](https://github.com/zenml-io/zenml/tree/main/examples).

AWS and Azure support is on the horizon, and we'd love your feedback on the
current setup. If you like what you see, leave us a
[star at the GitHub repo](https://github.com/zenml-io/zenml)!
